{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 4\n",
    "col = 4\n",
    "state_index_mapping = {}\n",
    "index_state_mapping = {}\n",
    "\n",
    "count = 0\n",
    "for i in range(row):\n",
    "    for j in range(col):\n",
    "        state_index_mapping[count] = (i, j)\n",
    "        count += 1\n",
    "\n",
    "count = 0\n",
    "for i in range(row):\n",
    "    for j in range(col):\n",
    "        index_state_mapping[(i, j)] = count\n",
    "        count += 1\n",
    "        \n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "gamma = 1\n",
    "\n",
    "def valid_state(i, j):\n",
    "    return 0 <= i < row and 0 <= j < col\n",
    "\n",
    "# starting with equiprobable actions\n",
    "policy_pie = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, 2: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, 3: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, 4: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, 5: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, 6: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, 7: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, 8: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, 9: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, 10: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, 11: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, 12: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, 13: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, 14: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}}\n",
      "{0: (0, 0), 1: (0, 1), 2: (0, 2), 3: (0, 3), 4: (1, 0), 5: (1, 1), 6: (1, 2), 7: (1, 3), 8: (2, 0), 9: (2, 1), 10: (2, 2), 11: (2, 3), 12: (3, 0), 13: (3, 1), 14: (3, 2), 15: (3, 3)}\n",
      "{(0, 0): 0, (0, 1): 1, (0, 2): 2, (0, 3): 3, (1, 0): 4, (1, 1): 5, (1, 2): 6, (1, 3): 7, (2, 0): 8, (2, 1): 9, (2, 2): 10, (2, 3): 11, (3, 0): 12, (3, 1): 13, (3, 2): 14, (3, 3): 15}\n"
     ]
    }
   ],
   "source": [
    "print(policy_pie)\n",
    "print(state_index_mapping)\n",
    "print(index_state_mapping)\n",
    "terminal_states = [(0,0), (row-1, col-1)]\n",
    "reward = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state_value(row, col, old_state_values):\n",
    "    for i in range(row):\n",
    "        for j in range(col):\n",
    "            print(round(old_state_values[index_state_mapping[(i, j)]], 0), end = \" \")\n",
    "        print()\n",
    "    print()\n",
    "\n",
    "def current_best_action(policy_pie, state):\n",
    "    maxi = 0\n",
    "    maxi_index = 0\n",
    "    for action in range(4):\n",
    "        if(policy_pie[state][action] >= maxi):\n",
    "            maxi = policy_pie[state][action] \n",
    "            maxi_index = action\n",
    "    return maxi_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy iteration\n",
    "# I didn't need to fix bug as list.index() returns the first max value in case of multiple same values \n",
    "def policy_iteration(num_states, theta, tolerance):\n",
    "    old_state_values = np.zeros(num_states) # Initialization\n",
    "    new_state_values = np.zeros(num_states)\n",
    "    \n",
    "    # policy evaluation\n",
    "    for i in range(1, row*col-1):\n",
    "        policy_pie[i] = {}\n",
    "        for action in range(4):\n",
    "            policy_pie[i][action] = (1/4)\n",
    "    count = 0\n",
    "    flag = True\n",
    "    while True and flag:\n",
    "        print(count)\n",
    "        count += 1\n",
    "        while True:\n",
    "            delta = 0\n",
    "            # count +=1\n",
    "            for state in range(1, row*col-1):\n",
    "                for action in range(4):\n",
    "                    next_state_i = state_index_mapping[state][0] + actions[action][0] \n",
    "                    next_state_j = state_index_mapping[state][1] + actions[action][1]\n",
    "                    if valid_state(next_state_i, next_state_j):\n",
    "                        new_state_values[state] += policy_pie[state][action]*(1)*(reward + gamma*old_state_values[index_state_mapping[(next_state_i, next_state_j)]])\n",
    "                    else:\n",
    "                        new_state_values[state] += policy_pie[state][action]*(1)*(reward + gamma*old_state_values[state]) \n",
    "                delta = max(delta, abs(old_state_values[state]-new_state_values[state]))\n",
    "            old_state_values = np.copy(new_state_values)\n",
    "            new_state_values = np.zeros(num_states)\n",
    "            # print(count, delta)   \n",
    "            if(delta < theta):\n",
    "                show_state_value(row, col, old_state_values)\n",
    "                break\n",
    "        # policy improvement\n",
    "        while True:\n",
    "            policy_stable = True\n",
    "            for state in range(1, row*col-1):\n",
    "                old_action = current_best_action(policy_pie, state)\n",
    "                state_action_value = []\n",
    "                for action in range(4):\n",
    "                    next_state_i = state_index_mapping[state][0] + actions[action][0] \n",
    "                    next_state_j = state_index_mapping[state][1] + actions[action][1]\n",
    "                    if valid_state(next_state_i, next_state_j):\n",
    "                        state_action_value.append(reward + gamma*old_state_values[index_state_mapping[(next_state_i, next_state_j)]])\n",
    "                    else:\n",
    "                        state_action_value.append(reward + gamma*old_state_values[state])\n",
    "                # updating policy_pie\n",
    "                optimal_action = state_action_value.index(max(state_action_value))\n",
    "                policy_pie[state][optimal_action] = 1\n",
    "                for action in range(4):\n",
    "                    if(optimal_action != action):\n",
    "                        policy_pie[state][action] = 0\n",
    "                if old_action != optimal_action:\n",
    "                    # print(\"yes\")\n",
    "                    policy_stable = False\n",
    "            if policy_stable:\n",
    "                show_state_value(row, col, old_state_values)\n",
    "                flag = False\n",
    "                break\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(num_states, theta, tolerance):\n",
    "    old_state_values = np.zeros(num_states) # Initialization\n",
    "    new_state_values = np.zeros(num_states)\n",
    "    for i in range(1, row*col-1):\n",
    "        policy_pie[i] = {}\n",
    "        for action in range(4):\n",
    "            policy_pie[i][action] = (1/4)\n",
    "    # policy evaluation\n",
    "    count = 0\n",
    "    flag = True\n",
    "    while True and flag:       \n",
    "        for iterator in range(1):\n",
    "            delta = 0\n",
    "            # count +=1\n",
    "            for state in range(1, row*col-1):\n",
    "                for action in range(4):\n",
    "                    next_state_i = state_index_mapping[state][0] + actions[action][0] \n",
    "                    next_state_j = state_index_mapping[state][1] + actions[action][1]\n",
    "                    if valid_state(next_state_i, next_state_j):\n",
    "                        new_state_values[state] += policy_pie[state][action]*(1)*(reward + gamma*old_state_values[index_state_mapping[(next_state_i, next_state_j)]])\n",
    "                    else:\n",
    "                        new_state_values[state] += policy_pie[state][action]*(1)*(reward + gamma*old_state_values[state]) \n",
    "                delta = max(delta, abs(old_state_values[state]-new_state_values[state]))\n",
    "            old_state_values = np.copy(new_state_values)\n",
    "            new_state_values = np.zeros(num_states)\n",
    "            if(delta < theta):\n",
    "                flag = False\n",
    "                break\n",
    "        print(\"values after iteration \", count)\n",
    "        count += 1\n",
    "        show_state_value(row, col, old_state_values)\n",
    "        if(flag == False):\n",
    "            break\n",
    "        for iterator in range(1):\n",
    "            for state in range(1, row*col-1):\n",
    "                # old_action = current_best_action(policy_pie, state)\n",
    "                state_action_value = []\n",
    "                for action in range(4):\n",
    "                    next_state_i = state_index_mapping[state][0] + actions[action][0] \n",
    "                    next_state_j = state_index_mapping[state][1] + actions[action][1]\n",
    "                    if valid_state(next_state_i, next_state_j):\n",
    "                        state_action_value.append(reward + gamma*old_state_values[index_state_mapping[(next_state_i, next_state_j)]])\n",
    "                    else:\n",
    "                        state_action_value.append(reward + gamma*old_state_values[state])\n",
    "                # updating policy_pie\n",
    "                optimal_action = state_action_value.index(max(state_action_value))\n",
    "                policy_pie[state][optimal_action] = 1\n",
    "                for action in range(4):\n",
    "                    if(optimal_action != action):\n",
    "                        policy_pie[state][action] = 0\n",
    "                # if old_action != optimal_action:\n",
    "                #     # print(\"yes\")\n",
    "                #     policy_stable = False\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0 -14.0 -20.0 -22.0 \n",
      "-14.0 -18.0 -20.0 -20.0 \n",
      "-20.0 -20.0 -18.0 -14.0 \n",
      "-22.0 -20.0 -14.0 0.0 \n",
      "\n",
      "1\n",
      "0.0 -1.0 -2.0 -3.0 \n",
      "-1.0 -2.0 -3.0 -2.0 \n",
      "-2.0 -3.0 -2.0 -1.0 \n",
      "-3.0 -2.0 -1.0 0.0 \n",
      "\n",
      "2\n",
      "0.0 -1.0 -2.0 -3.0 \n",
      "-1.0 -2.0 -3.0 -2.0 \n",
      "-2.0 -3.0 -2.0 -1.0 \n",
      "-3.0 -2.0 -1.0 0.0 \n",
      "\n",
      "0.0 -1.0 -2.0 -3.0 \n",
      "-1.0 -2.0 -3.0 -2.0 \n",
      "-2.0 -3.0 -2.0 -1.0 \n",
      "-3.0 -2.0 -1.0 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy_iteration(num_states = row*col, theta= 1e-6, tolerance= 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {0: 0, 1: 0, 2: 1, 3: 0}, 2: {0: 0, 1: 0, 2: 1, 3: 0}, 3: {0: 0, 1: 1, 2: 0, 3: 0}, 4: {0: 1, 1: 0, 2: 0, 3: 0}, 5: {0: 1, 1: 0, 2: 0, 3: 0}, 6: {0: 1, 1: 0, 2: 0, 3: 0}, 7: {0: 0, 1: 1, 2: 0, 3: 0}, 8: {0: 1, 1: 0, 2: 0, 3: 0}, 9: {0: 1, 1: 0, 2: 0, 3: 0}, 10: {0: 0, 1: 1, 2: 0, 3: 0}, 11: {0: 0, 1: 1, 2: 0, 3: 0}, 12: {0: 1, 1: 0, 2: 0, 3: 0}, 13: {0: 0, 1: 0, 2: 0, 3: 1}, 14: {0: 0, 1: 0, 2: 0, 3: 1}}\n"
     ]
    }
   ],
   "source": [
    "print(policy_pie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value after iteration  0\n",
      "0.0 -1.0 -1.0 -1.0 \n",
      "-1.0 -1.0 -1.0 -1.0 \n",
      "-1.0 -1.0 -1.0 -1.0 \n",
      "-1.0 -1.0 -1.0 0.0 \n",
      "\n",
      "value after iteration  1\n",
      "0.0 -1.0 -2.0 -2.0 \n",
      "-1.0 -2.0 -2.0 -2.0 \n",
      "-2.0 -2.0 -2.0 -1.0 \n",
      "-2.0 -2.0 -1.0 0.0 \n",
      "\n",
      "value after iteration  2\n",
      "0.0 -1.0 -2.0 -3.0 \n",
      "-1.0 -2.0 -3.0 -2.0 \n",
      "-2.0 -3.0 -2.0 -1.0 \n",
      "-3.0 -2.0 -1.0 0.0 \n",
      "\n",
      "value after iteration  3\n",
      "0.0 -1.0 -2.0 -3.0 \n",
      "-1.0 -2.0 -3.0 -2.0 \n",
      "-2.0 -3.0 -2.0 -1.0 \n",
      "-3.0 -2.0 -1.0 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "value_iteration(num_states = row*col, theta= 1e-6, tolerance= 1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
